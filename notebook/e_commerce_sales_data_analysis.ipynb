{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b78add1-aefa-47fe-b9ab-e2cfde4cd951",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "installing required modules"
    }
   },
   "outputs": [],
   "source": [
    "!pip --disable-pip-version-check instal -r requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34afcb71-c22b-49f2-ba75-f26fa2ab0360",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import all dependent modules"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "src_path = os.path.abspath('/Workspace/Repos/srivastav_amol@yahoo.com/Ecommerce-Sales-Data-Processing/src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "from src.config import read_config\n",
    "from src.data_quality_checks import data_quality_checks\n",
    "from src.create_raw_tables import clean_and_save_tables\n",
    "from src.create_enriched_tables import enrich_and_save_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4614c699-b16c-4a9d-a953-880b6e22edc3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read Configurations"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read the config file\n",
    "\n",
    "config = read_config(config_path=\"../configs/config.yaml\")\n",
    "\n",
    "data_quality_checks(\n",
    "    config[\"customers_path\"],\n",
    "    config[\"products_path\"],\n",
    "    config[\"orders_path\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21adab63-8e13-4f0c-91c5-a86ec86cca33",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create dataframe for Customers , Products and Orders"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define required columns for customers\n",
    "required_columns = [\n",
    "    \"Customer ID\",\n",
    "    \"Customer Name\",\n",
    "    \"email\",\n",
    "    \"phone\",\n",
    "    \"address\"\n",
    "]\n",
    "\n",
    "# Read the Excel file into a pandas DataFrame\n",
    "pdf = pd.read_excel(\n",
    "    config[\"customers_path\"],\n",
    "    engine=\"openpyxl\"\n",
    ")\n",
    "\n",
    "# Ensure 'phone' column contains only string values\n",
    "pdf[\"phone\"] = pdf[\"phone\"].astype(str)\n",
    "\n",
    "# Check for nulls in key columns and handle them\n",
    "null_columns = pdf[required_columns].isnull().any()\n",
    "if null_columns.any():\n",
    "    missing = list(null_columns[null_columns].index)\n",
    "    warnings.warn(\n",
    "        f\"Null values found in columns: {missing}. Filling with default values.\"\n",
    "    )\n",
    "    if \"Customer Name\" in missing:\n",
    "        pdf[\"Customer Name\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# Create a Spark DataFrame from the pandas DataFrame\n",
    "df_customers = spark.createDataFrame(pdf)\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame\n",
    "df_products = spark.read.csv(\n",
    "    config[\"products_path\"],\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df_products = df_products.dropDuplicates()\n",
    "# Read the JSON file into a Spark DataFrame\n",
    "df_orders = spark.read.option(\"multiline\", \"true\").json(\n",
    "    config[\"orders_path\"]\n",
    ")\n",
    "\n",
    "df_orders = df_orders.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "323044da-7d4c-4410-98fb-d0e34b49628f",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755429708443}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Validate dataframe before ingestion"
    }
   },
   "outputs": [],
   "source": [
    "# check if null Customer Name is present in dataframe\n",
    "\n",
    "df_customers.filter(df_customers[\"Customer Name\"]==\"Unknown\").display()\n",
    "\n",
    "# Check duplicate rows in df_products\n",
    "total_rows_products = df_products.count()\n",
    "unique_rows_products = df_products.dropDuplicates().count()\n",
    "duplicate_rows_products = total_rows_products - unique_rows_products\n",
    "print(\"Duplicate rows in df_products:\", duplicate_rows_products)\n",
    "\n",
    "# Check duplicate rows in df_orders\n",
    "total_rows_orders = df_orders.count()\n",
    "unique_rows_orders = df_orders.dropDuplicates().count()\n",
    "duplicate_rows_orders = total_rows_orders - unique_rows_orders\n",
    "print(\"Duplicate rows in df_orders:\", duplicate_rows_orders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "525257e9-e0ef-4c4a-a0c0-b991628acd11",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "create raw tables"
    }
   },
   "outputs": [],
   "source": [
    "# Call the function to clean and save raw tables\n",
    "clean_and_save_tables(df_customers, df_products, df_orders, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b542307-9d8a-414e-b7c1-ad0c42396ecf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "create enriched tables"
    }
   },
   "outputs": [],
   "source": [
    "enrich_and_save_tables(*clean_and_save_tables(df_customers, df_products, df_orders, config), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfeb7c06-bfe4-4d61-b109-fb1061e37008",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "validate enriched data"
    }
   },
   "outputs": [],
   "source": [
    "enriched_dfs = enrich_and_save_tables(*clean_and_save_tables(df_customers, df_products, df_orders, config), config)\n",
    "for df in enriched_dfs:\n",
    "    display(df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4208990f-10d8-4c75-b4db-c42c64e81e0b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Profit by Year"
    }
   },
   "outputs": [],
   "source": [
    "# Profit by Year\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        year,\n",
    "        SUM(total_profit) AS total_profit\n",
    "    FROM amol_uc.default.profit_agg_by_year_category_subcat_customer\n",
    "    GROUP BY year\n",
    "    ORDER BY year\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e42792-3f54-4e9b-8ddd-d12370df8d43",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755434271178}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Profit by Year + Product Category"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        year,\n",
    "        category,\n",
    "        SUM(total_profit) AS total_profit\n",
    "\n",
    "    FROM amol_uc.default.profit_agg_by_year_category_subcat_customer\n",
    "    GROUP BY year, category\n",
    "    ORDER BY year, category\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca2e3571-5742-4490-9b6c-2df107c9c455",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755435572535}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Profit by Customer"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "    customer_name,\n",
    "    COUNT(DISTINCT sub_category) AS total_sub_categories,\n",
    "    ROUND(SUM(total_profit), 2) AS total_profit\n",
    "FROM amol_uc.default.profit_agg_by_year_category_subcat_customer\n",
    "GROUP BY customer_name\n",
    "ORDER BY total_profit DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5433fcb4-6632-4822-b55d-05650aad1ad0",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755435640617}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Profit by Customer + Year"
    }
   },
   "outputs": [],
   "source": [
    "# Profit by Customer + Year\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        customer_name,\n",
    "        year,\n",
    "        ROUND(SUM(total_profit), 2) AS total_profit\n",
    "    FROM amol_uc.default.profit_agg_by_year_category_subcat_customer\n",
    "    GROUP BY customer_name, year\n",
    "    ORDER BY customer_name, year\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0a5631a-c861-4386-becd-0874456323f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "drop tables"
    }
   },
   "outputs": [],
   "source": [
    "# # Get table names as a list using DataFrame operations only\n",
    "# tables_df = spark.sql(\"SHOW TABLES IN amol_uc.default\").select(\"tableName\")\n",
    "# tables = [row.tableName for row in tables_df.collect()]\n",
    "\n",
    "# # Drop each table\n",
    "# for table in tables:\n",
    "#     spark.sql(f\"DROP TABLE amol_uc.default.{table}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8032329813686594,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "e_commerce_sales_data_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
